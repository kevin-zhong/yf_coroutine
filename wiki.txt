2013/03/14
coroutine功能基本完善，单元测试全部通过

2013/03/14 18:58
增加了 recvn, sendn 两个非常重要的自定义 api socket 函数；

2013/03/15 18:53
一天半终于把 socket_ext 中的复用socket api定下来了，感觉还是有点诡异；
呃，暂时也没好办法了；

2013/03/16 19:20
一直在纠结怎样把yifei中的hash这个数据结构加上去；
受到nginx的hash影响，差点一样画葫芦也搞个类似的功能复杂的hash加上去，
搞了一个上午，发现掉进nginx的彀中了；把内存分配管理和hash的逻辑全混到
一起了，硬是花了一个上午才把头文件定下来，数数代码行数，30多行不到，
改了n次，最后定型，然后吃饭；吃饭的时候，猛然回过神来，这玩意怎么越看
越别扭，用起来肯定也很难受；怎么和 stl 的hash一样了；再对比下已有的
rbtree，猛然醒悟，我在用c++的思想在写c的代码；在c里面内存管理不是数据
结构应该干的活；
最后推到重来，很快头文件定义api都好了，感觉是那么自然轻松，因为再也不
需要被怎么分配内存，怎么安排内存恶心到了；

2013/03/20 20:10
连接请求并发复用的代码写了几天了，写了大概700多行，最初的架构设想是
每个连接专门起一个专用的coroutine来读数据；后来觉得这个有点浪费，于是乎
想不起coroutine，就采用原生的evt回调来出来；代码越写越多，正常流程倒是
也不难，难的是处理各类异常情况，发现没有coroutine的支持，代码越来越难
写（就是回调和evt越来越多。。。一堆的on什么什么等callback函数）差点崩溃了；
狠下心来，推翻现在的架构还是回到之前的架构上去；简单明了是其次，关键是处理
异常要简单的多了；现在知道nginx的痛苦了吧，呃，能把那么长的流程全部用回调
写出来，确实非常困难和恶心；coroutine，解决痛苦的利器；！


2013/03/25 21:54
觉得给socket用的 chain 用起来非常别扭，而且在有些情况下满足不了需求
于是想开发自己的的socket专用缓冲buf，要满足几点：
1，能自动伸缩，即能扩容，能缩减，以适应socket中不同的缓冲大小需求；
2，尽量少memcpy，强效率；
3，能像数组那样快速定位；
4，提供文件类型的接口，比如 ftell，fseek，ftruncate，fread，fwrite；
5，能直接提供内存片区，以供 socket 直接写（即socket读取的数据直接写入这个buf
	避免读取后再调用buf write）
	
于是开发，接口改了又改，实现发现越来越复杂，比想象中的要复杂很多；
周末两个晚上耗了6-7个小时才大体开发完毕；
然后写单元测试，发现单元测试也不简单，测试程序要写好，也不容易，好几种
情况下，发现都是测试程序写错了；

大体能跑通，但还是有问题；。。。

13/05/23 17:57:49 440 [./syscall_hook/yfr_socket_ext.c:891]-[debug]-[3086173968-503880678205]readv len=[22235,239909], recv ret=0
13/05/23 17:57:49 440 [./syscall_hook/yfr_socket_ext.c:978]-[warn]-[3086173968-503880678205]to close, read err, fd=18,tdp_fd=503880564760
13/05/23 17:57:49 440 [./base_struct/yf_mem_pool.c:78]-[debug]-[3086173968-503880678205]free: 0AC5CE80, unused: 2000
13/05/23 17:57:49 440 [./syscall_hook/yfr_socket_ext.c:1255]-[debug]-[3086173968-503880678205]rw error, type=0, delete from conn pool, rest conn num=4
13/05/23 17:57:49 440 [./mio_driver/event_in/yf_tm_event_in.c:112]-[debug]-[3086173968-503880678205]fd tm num=0, single tm num=243, addr=0932AC88
13/05/23 17:57:49 440 [./mio_driver/event_in/yf_tm_event_in.c:141]-[debug]-[3086173968-503880678205]far timer roll index=355, add timer index=479, pass_mod_ms=208, diff_ms=31952
13/05/23 17:58:21 443 [./syscall_hook/yfr_socket_ext.c:1354]-[debug]-[3086173968-503880678205]close tdp_fd=503880564760,fd=18
13/05/23 17:58:21 443 [./syscall_hook/yfr_syscall_socket.c:187]-[debug]-[3086173968-503880678205]close fd impl..18
13/05/23 17:58:21 443 [./syscall_hook/yfr_syscall_socket.c:188]-[warn]-[3086173968-503880678205]socket=18 not opened
13/05/23 17:58:21 443 [./coroutine/yfr_coroutine.c:323]-[debug]-[3086173968-503880678205]coroutine ended, id=503880678205, ready cnt=3, block cnt=11

//coroutine lost...
[kevin_zhong@localhost yf_coroutine]$ grep 508175508250 test/log/sock_svr.log | tail -10
13/05/23 17:52:36 016 [./syscall_hook/yfr_socket_ext.c:1138]-[debug]-[3086173968-508175508250]recv datagram{type=request, id=7366, alllen=192157}, tdp_fd=508175532046
13/05/23 17:52:36 016 [./coroutine/yfr_coroutine.c:436]-[debug]-[3086173968-508175508250]new coroutine created, id=4356466342680, ready cnt=1, block cnt=266
13/05/23 17:52:36 016 [./mio_driver/event_in/yf_fd_event_in.c:227]-[debug]-[3086173968-508175508250]register fd evt, fd=13, evt=read_evt
13/05/23 17:52:36 128 [./mio_driver/yf_send_recv.c:82]-[debug]-[3086173968-508175508250]recv: fd:13 -1 of 262144
13/05/23 17:52:36 128 [./mio_driver/yf_send_recv.c:82]-[error]-[3086173968-508175508250]recv() failed (104: Connection reset by peer)
13/05/23 17:52:36 128 [./syscall_hook/yfr_socket_ext.c:891]-[debug]-[3086173968-508175508250]readv len=[245988,16156], recv ret=-1
13/05/23 17:52:36 128 [./syscall_hook/yfr_socket_ext.c:978]-[warn]-[3086173968-508175508250]to close, read err, fd=13,tdp_fd=508175532046
13/05/23 17:52:36 128 [./base_struct/yf_mem_pool.c:78]-[debug]-[3086173968-508175508250]free: 0A815FF0, unused: 2008
13/05/23 17:52:36 128 [./syscall_hook/yfr_socket_ext.c:1255]-[debug]-[3086173968-508175508250]rw error, type=0, delete from conn pool, rest conn num=2
13/05/23 17:52:36 128 [./base_struct/yf_node_pool.c:73]-[debug]-[3086173968-508175508250]total_num=1024, after alloc 1369374808, free=1018, alloc_ptr=0934308C


2014/06/24 
linux 下 syshook 只能针对外部的接口访问，但对于系统内部的函数，如果此函数内部调用了其他的函数，是完全无法hook的
比如：getaddreinfo 其内部其实也是由send，recv等等组成的，但你是完全无法hook里面的调用的，所以才必须直接hook getaddreinfo
同样的道理，在测试中，发现有部分函数完全无法hook：

1403586210.206412 socket(PF_NETLINK, SOCK_RAW, 0) = 9
1403586210.206503 bind(9, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0
1403586210.206598 getsockname(9, {sa_family=AF_NETLINK, pid=2312, groups=00000000}, [37095392017383436]) = 0
1403586210.206746 gettimeofday({1403586210, 206766}, NULL) = 0
1403586210.206843 sendto(9, "\24\0\0\0\22\0\1\3\242\6\251S\0\0\0\0\0\0\0\0", 20, 0, {sa_family=AF_NETLINK, pid=0, groups=00000000},
12) = 20
1403586210.206977 recvmsg(9, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{"\360\0\0\0\20\0\2\0\242\6\2
51S\10\t\0\0\0\0\4\3\1\0\0\0"..., 4096}], msg_controllen=0, msg_flags=0}, 0) = 972
1403586210.207132 recvmsg(9, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{"\24\0\0\0\3\0\2\0\242\6\251
S\10\t\0\0\0\0\0\0\1\0\0\0I"..., 4096}], msg_controllen=0, msg_flags=0}, 0) = 20 
1403586210.207282 sendto(9, "\24\0\0\0\26\0\1\3\243\6\251S\0\0\0\0\0\357\v\224", 20, 0, {sa_family=AF_NETLINK, pid=0, groups=0000000
0}, 12) = 20
1403586210.207401 recvmsg(9, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{"0\0\0\0\24\0\2\0\243\6\251S
\10\t\0\0\2\10\200\376\1\0\0"..., 4096}], msg_controllen=0, msg_flags=0}, 0) = 108
1403586210.207552 recvmsg(9, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{"\24\0\0\0\3\0\2\0\243\6\251
S\10\t\0\0\0\0\0\0\1\0\0\0\10"..., 4096}], msg_controllen=0, msg_flags=0}, 0) = 20
1403586210.207697 close(9)  

且，如果用gdb break 这几个函数，均直接跳过了，因为gdb break的函数实际上 yf_coroutine 里面的函数，但实际这几个根本就没被hook，其指向的是原来so里面的函数   


仔细一想，这个其实是说的通的，因为hook本来就是借助so的动态加载才实现的
对于so里面的函数，如果其调用自身内部的函数，其实应该是差不多绑定了，所以gdb才break不掉
怎么证实呢：完全可以写个so，里面有个函数A调用so里面的一个函数B，然后在另外一个so hook函数B，看下效果


2015/07/09
原来coroutine的堆栈保护一直有问题
mprotect 一直没有起作用，之前还怀疑这个函数的有效性
后来有次strace程序，发现 mprotect 返回都是失败，strace发现 mprotect 的地址都有问题
后来才发现这个宏有问题，yf_pagesize 是 uint，取-后，其实不是负数，而是一个很大的正整数。。。
#define _yfr_coroutine_s2m(s)  yf_mem_off(s, -(yf_s32_t)yf_pagesize)

更改之后，正常工作，发现如果堆栈要求内存超过固定大小，即发生段错误，如下代码：
char buffer[kMaxBufferSize] = {0}; //在栈上分配了 64k 的数据，而整个栈才64k，所以立即崩掉了，gdb查看core文件发现：

Program terminated with signal 11, Segmentation fault.
#0  0x00007f1f1ed9e14b in memset () from /lib64/libc.so.6
这里调用 memset，而内存已经保护了，自然core了


2015/10/15
mysql连接本地的coroutine会有问题

2016/01/18
所有测试均在同一台 C1 的机器上进行
--8  Intel(R) Xeon(R) CPU           X3440  @ 2.53GHz
--cache size      : 8192 KB
-- physical(1) {cpu cores(4), core ids(8)} // cpu cores -4, siblings - 8

协程的切换效率到底怎么样，对业务层的影响又是多大?
经过测试: 
makecontext 裸用: 一秒切换大约是 240万次左右
coroutine: 一秒切换大约是 1200万次左右
libco: 一秒切换大约是 850万次左右
// 测试结果与libco自测差不多：在C1机型上单线程,一个来回 co_swapcontext 1千万次750ms, libc swapcontext 2550ms(3.4倍)

按一个svr，一秒服务5万请求，一次请求3个远程调用(外加一次进入退出的切换算1次)，一秒切换量为：5*4=20万
如果用 makecontext 来实现，则协程对业务的影响是：cpu额外占用 20万/240万 = 8.3% 的cpu
如果用 coroutine 来实现，则协程对业务的影响是：cpu额外占用 20万/1200万 = 1.67% 的cpu
如果用 libco 来实现...

coroutine 对业务的影响是非常小的

2016/01/19
今天测试了模拟业务请求过来，从创建协程，协程切换（里面切换三次）, 然后退出的过程
这是比较真实的模拟
经过测试：
avg: 238 million coroutine [create+run+exit] times per sec
按一个svr，一秒服务5万请求，一次请求3次远程调用（即和模拟一致）
则协程对业务的影响是：cpu总体额外占用：5万/238万 = 2.10%
性能是非常高效的

makecontext 做底层实现：10.43%，影响已经比较可观了(1/3的cpu耗在信号处理的syscall调用上面)

对比了 libco 的测试：
经过测试：
avg: 149 million routine times per sec
按一个svr，一秒服务5万请求，一次请求3次远程调用（即和模拟一致）
则协程对业务的影响是：cpu总体额外占用：5万/149万 = 3.36%

协程效率：238/149~=1.60plus

每个请求一个协程，协程id是完全(libco在这点上不好用)


2016/01/19 22:15:08
包长为：32字节, client 并发：900连接, 延时超过3s(单向1.5s)即失败，在失败率为0的情况下：
example_echocli -> (./example_echosvr 0.0.0.0 9987) -> (./example_echosvr 0.0.0.0 9986)

......
time 1453253539 Succ Cnt 51439 Fail Cnt 0
time 1453253540 Succ Cnt 52040 Fail Cnt 0
time 1453253541 Succ Cnt 57553 Fail Cnt 0
time 1453253542 Succ Cnt 57630 Fail Cnt 0
time 1453253543 Succ Cnt 57195 Fail Cnt 0
time 1453253544 Succ Cnt 57226 Fail Cnt 0
time 1453253545 Succ Cnt 56649 Fail Cnt 0
time 1453253546 Succ Cnt 57359 Fail Cnt 0
time 1453253547 Succ Cnt 56674 Fail Cnt 0
time 1453253548 Succ Cnt 56908 Fail Cnt 0
time 1453253549 Succ Cnt 57827 Fail Cnt 0
time 1453253550 Succ Cnt 58275 Fail Cnt 0
......
avg: 5.4万 qps，无丢包

example_echocli(10.185.12.41) -> (yf_echo_svr 0.0.0.0 9986 0.0.0.0 9987) -> (./yf_echo_svr 0.0.0.0 9987)
......
time 1453250340 Succ Cnt 83517 Fail Cnt 0
time 1453250355 Succ Cnt 74113 Fail Cnt 0
time 1453250356 Succ Cnt 74076 Fail Cnt 0
time 1453250357 Succ Cnt 74262 Fail Cnt 0
time 1453250358 Succ Cnt 73549 Fail Cnt 0
time 1453250359 Succ Cnt 73995 Fail Cnt 0
time 1453250468 Succ Cnt 76298 Fail Cnt 0
time 1453250469 Succ Cnt 77000 Fail Cnt 0
time 1453250482 Succ Cnt 82295 Fail Cnt 0
time 1453250483 Succ Cnt 82021 Fail Cnt 0
time 1453250484 Succ Cnt 82369 Fail Cnt 0
......

avg: 7.6万 qps
cpu 见截图
总体效率：7.6/5.4~=1.41plus


2016/04/26
yf_coroutine + 外部一些log库结合的情况下，无法支持 memcached client
但如果是 yf_log 本身的 log 库就没问题(比如：写个简单的不依赖那个特殊log库的测试程序，是完全正常的)
经 gdb 单步执行，在 recvfrom 这里找到了问题
errno 本来在 recvfrom 里面是：EAGAIN，但到了memcached 的调用处(io.cc:memcached_io_read 的 ::recv 处)
errno 竟然被设置为了 0, 然后memcached 关了连接...

所以 errno 的设置一定要在 return 之前，防止设置后，调用其他函数被改写


2016/08/10
今天终于将test程序全部都改成了静态链接 libyf_coroutine.a
因为今天在一台机器上测试的时候，发现动态链接ld后，报：
unresolvable R_X86_64_PC32 relocation against symbol _greenlet_savecontext
unresolvable R_X86_64_PC32 relocation against symbol _greenlet_fcall_end
dynamic variable is zero size _greenlet_savecontext/_greenlet_fcall_end

disas _coroutine_fcall_child1 竟然发现：_greenlet_savecontext 地址为 0x0...
0x0000000000407f7f <_Z23_coroutine_fcall_child1iPiPvS_+173>:    mov    0xffffffffffffffb0(%rbp),%rax
0x0000000000407f83 <_Z23_coroutine_fcall_child1iPiPvS_+177>:    add    $0x38,%rax
0x0000000000407f87 <_Z23_coroutine_fcall_child1iPiPvS_+181>:    mov    %rax,%rdi
0x0000000000407f8a <_Z23_coroutine_fcall_child1iPiPvS_+184>:    callq  0x0
0x0000000000407f8f <_Z23_coroutine_fcall_child1iPiPvS_+189>:    test   %eax,%eax
0x0000000000407f91 <_Z23_coroutine_fcall_child1iPiPvS_+191>:    sete   %al
0x0000000000407f94 <_Z23_coroutine_fcall_child1iPiPvS_+194>:    test   %al,%al
0x0000000000407f96 <_Z23_coroutine_fcall_child1iPiPvS_+196>:    je     0x4080c2 <_Z23_coroutine_fcall_child1iPiPvS_+496>

估计是c++调用汇编函数链接问题
懒得麻烦了，改成静态链接，ok了：
0x0000000000407f7f <_Z23_coroutine_fcall_child1iPiPvS_+173>:    mov    0xffffffffffffffb0(%rbp),%rax
0x0000000000407f83 <_Z23_coroutine_fcall_child1iPiPvS_+177>:    add    $0x38,%rax
0x0000000000407f87 <_Z23_coroutine_fcall_child1iPiPvS_+181>:    mov    %rax,%rdi
0x0000000000407f8a <_Z23_coroutine_fcall_child1iPiPvS_+184>:    callq  0x45255c <_greenlet_savecontext>
0x0000000000407f8f <_Z23_coroutine_fcall_child1iPiPvS_+189>:    test   %eax,%eax
0x0000000000407f91 <_Z23_coroutine_fcall_child1iPiPvS_+191>:    sete   %al
0x0000000000407f94 <_Z23_coroutine_fcall_child1iPiPvS_+194>:    test   %al,%al
0x0000000000407f96 <_Z23_coroutine_fcall_child1iPiPvS_+196>:    je     0x4080c2 <_Z23_coroutine_fcall_child1iPiPvS_+496>

幸亏懂点汇编....
